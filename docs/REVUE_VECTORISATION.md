# Revue approfondie de la vectorisation
**Date** : 06 f√©vrier 2026
**Contexte** : Comparaison entre l'impl√©mentation pr√©c√©dente (FastAPI + ChromaDB) et l'impl√©mentation actuelle (Django + pgvector)

---

## üìä Tableau comparatif

| Aspect | Ancienne version (IMPLEMENTATION_CHATBOT_RAG.md) | Version actuelle | Statut |
|--------|--------------------------------------------------|------------------|--------|
| **Framework** | FastAPI + ChromaDB | Django + pgvector | ‚úÖ Migration r√©ussie |
| **Extraction PDF** | PyMuPDF avec num√©ros de page | Identique | ‚úÖ OK |
| **Extraction DOCX** | python-docx, paragraphes | Identique | ‚úÖ OK |
| **Extraction XLSX** | Simple, sans statistiques | **Avec statistiques** (lignes, colonnes) | ‚úÖ Am√©lior√© |
| **Chunking** | 1000 chars, overlap 200 | Identique | ‚úÖ OK |
| **Filtrage chunks vides** | ‚ùå Non | ‚úÖ Oui (< 50 chars) | ‚úÖ Am√©lior√© |
| **M√©tadonn√©es chunks** | ‚úÖ source, chunk_index, char_count | ‚ùå Seulement content | ‚ö†Ô∏è **R√©gression** |
| **Stockage m√©tadonn√©es** | ChromaDB (collection.metadata) | Mod√®le Django (limit√©) | ‚ö†Ô∏è **Limitation** |
| **Gestion pages vides PDF** | Cr√©e des chunks vides | Filtre automatiquement | ‚úÖ Am√©lior√© |

---

## üî¥ Probl√®me principal identifi√© : Perte des m√©tadonn√©es

### Ancienne version

```python
def chunk_text(self, text: str, filename: str) -> list[dict]:
    """D√©coupe le texte en chunks avec m√©tadonn√©es."""
    chunks = self.text_splitter.split_text(text)

    return [
        {
            "content": chunk,
            "metadata": {
                "source": filename,        # Nom du fichier
                "chunk_index": i,          # Position dans le document
                "char_count": len(chunk)   # Taille du chunk
            }
        }
        for i, chunk in enumerate(chunks)
    ]
```

### Version actuelle

```python
def chunk_text(self, text: str, min_chunk_size: int = 50) -> List[Dict]:
    """D√©coupe un texte en chunks avec chevauchement."""
    raw_chunks = self.text_splitter.split_text(text)
    filtered_chunks = [
        {"content": chunk}  # ‚ùå Pas de m√©tadonn√©es !
        for chunk in raw_chunks
        if len(chunk.strip()) >= min_chunk_size
    ]
    return filtered_chunks
```

**Impact** :
- ‚ùå Impossible de tracer la source exacte d'un chunk
- ‚ùå Pas de r√©f√©rence √† la position dans le document
- ‚ùå Difficile de citer pr√©cis√©ment (ex: "page 5, paragraphe 3")

---

## üóÑÔ∏è Mod√®le de donn√©es : Limitations actuelles

### Mod√®le DocumentChunk actuel

```python
class DocumentChunk(models.Model):
    document = models.ForeignKey(Document, on_delete=models.CASCADE)
    chunk_index = models.PositiveIntegerField()
    content = models.TextField()
    embedding = VectorField(dimensions=1024)
```

**Champs manquants** pour une meilleure tra√ßabilit√© :
- `source_filename` : Nom du fichier d'origine
- `char_count` : Longueur du chunk
- `metadata` : JSONField pour stocker des infos suppl√©mentaires (page, feuille Excel, etc.)

---

## üîç Extraction XLSX : Analyse d√©taill√©e

### Ancienne version (simple)

```python
def _extract_xlsx(self, file_path: str) -> str:
    wb = openpyxl.load_workbook(file_path)
    text = ""
    for sheet_name in wb.sheetnames:
        sheet = wb[sheet_name]
        text += f"\n\n--- Feuille: {sheet_name} ---\n\n"
        for row in sheet.iter_rows(values_only=True):
            text += " | ".join([str(cell) for cell in row if cell]) + "\n"
    return text
```

**Probl√®mes** :
- ‚ùå Pas d'info sur le nombre total de lignes
- ‚ùå Difficile de compter les enregistrements
- ‚ùå Pas de distinction en-t√™te / donn√©es

### Version actuelle (am√©lior√©e)

```python
def _extract_xlsx(self, file_path: str) -> str:
    wb = openpyxl.load_workbook(file_path)
    sheets = []

    for sheet_name in wb.sheetnames:
        sheet = wb[sheet_name]
        all_rows = list(sheet.iter_rows(values_only=True))

        # Statistiques
        total_rows = len(all_rows)
        data_rows = total_rows - 1 if total_rows > 0 else 0

        header = f"\n\n--- Feuille : {sheet_name} ---\n"
        header += f"STATISTIQUES : {data_rows} lignes de donn√©es, {total_rows} lignes au total"

        # En-t√™tes de colonnes
        if all_rows:
            first_row = all_rows[0]
            columns = [str(c) for c in first_row if c is not None]
            if columns:
                header += f", {len(columns)} colonnes\n"
                header += f"COLONNES : {' | '.join(columns)}\n"

        # Donn√©es
        for row in all_rows:
            rows.append(" | ".join(str(c) for c in row if c is not None))

        sheets.append(header + "\n".join(rows))

    return "".join(sheets)
```

**Avantages** :
- ‚úÖ Statistiques visibles dans le premier chunk
- ‚úÖ Permet de r√©pondre √† "Combien d'√©l√®ves ?" sans compter
- ‚úÖ Nom des colonnes explicite

**Mais** : Le probl√®me persiste si le fichier est tr√®s grand (> 600 chunks) car seuls 5 chunks sont r√©cup√©r√©s par requ√™te RAG.

---

## üìÑ Extraction PDF : Pages vides

### Test avec spe641_annexe_1063085.pdf

**R√©sultat** :
- 94 chunks cr√©√©s au total
- 17 chunks vides (pages sans texte extractible)
- Chunks vides : [6, 11, 16, 22, 27, 33, 37, 42, 47, 52, 58, 63, 69, 74, 79, 84, 89]

**Causes possibles** :
1. Pages scann√©es (images) sans couche OCR
2. Pages de couverture graphiques
3. Tableaux complexes non reconnus par PyMuPDF

**Solution actuelle** :
```python
# Filtrage automatique des chunks < 50 caract√®res
filtered_chunks = [
    {"content": chunk}
    for chunk in raw_chunks
    if len(chunk.strip()) >= min_chunk_size  # D√©faut: 50
]
```

‚úÖ **Efficace** : R√©duit de 94 √† 77 chunks utiles pour le PDF de test.

---

## üéØ Recommandations d'am√©lioration

### 1. Restaurer les m√©tadonn√©es dans chunk_text ‚≠ê **PRIORIT√â HAUTE**

```python
def chunk_text(self, text: str, filename: str, min_chunk_size: int = 50) -> List[Dict]:
    """D√©coupe un texte en chunks avec m√©tadonn√©es compl√®tes."""
    raw_chunks = self.text_splitter.split_text(text)

    filtered_chunks = []
    original_index = 0

    for chunk in raw_chunks:
        if len(chunk.strip()) >= min_chunk_size:
            filtered_chunks.append({
                "content": chunk,
                "metadata": {
                    "source": filename,
                    "chunk_index": len(filtered_chunks),
                    "original_index": original_index,
                    "char_count": len(chunk),
                    "is_filtered": False
                }
            })
        original_index += 1

    if len(filtered_chunks) < len(raw_chunks):
        logger.info(
            f"Filtr√© {len(raw_chunks) - len(filtered_chunks)} chunks vides "
            f"({len(filtered_chunks)} chunks conserv√©s)"
        )

    return filtered_chunks
```

**Avantages** :
- ‚úÖ Tra√ßabilit√© compl√®te de chaque chunk
- ‚úÖ Distinction entre index filtr√© et index original
- ‚úÖ Pr√©pare l'ajout futur de m√©tadonn√©es suppl√©mentaires

### 2. Enrichir le mod√®le DocumentChunk (Optionnel)

```python
class DocumentChunk(models.Model):
    document = models.ForeignKey(Document, on_delete=models.CASCADE, related_name='chunks')
    chunk_index = models.PositiveIntegerField()
    content = models.TextField()
    embedding = VectorField(dimensions=settings.EMBEDDING_DIMENSIONS)

    # Nouveaux champs
    char_count = models.PositiveIntegerField(default=0)
    metadata = models.JSONField(default=dict)  # Pour infos suppl√©mentaires (page, feuille, etc.)

    class Meta:
        ordering = ['chunk_index']
        unique_together = [('document', 'chunk_index')]
```

**Note** : N√©cessite une migration. √Ä faire uniquement si besoin r√©el.

### 3. Extraction PDF avanc√©e avec OCR (Futur)

Pour les pages scann√©es, ajouter Tesseract OCR :

```python
def _extract_pdf_with_ocr(self, file_path: str) -> str:
    """Extrait texte d'un PDF avec fallback OCR pour images."""
    import pytesseract
    from PIL import Image

    doc = fitz.open(file_path)
    pages = []

    for page_num, page in enumerate(doc, start=1):
        text = page.get_text()

        # Si page vide, tenter OCR
        if len(text.strip()) < 20:
            pix = page.get_pixmap()
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            text = pytesseract.image_to_string(img, lang='fra')

        pages.append(f"\n\n--- Page {page_num} ---\n\n{text}")

    doc.close()
    return "".join(pages)
```

‚ö†Ô∏è **Attention** : OCR est lent et consommateur de ressources. √Ä activer seulement si besoin.

### 4. Am√©liorer le RAG pour les grandes tables (XLSX)

**Probl√®me** : 796 √©l√®ves = 607 chunks, mais TOP_K = 5 ‚Üí contexte insuffisant.

**Solutions possibles** :

**A. Augmenter TOP_K dynamiquement**
```python
# Dans rag_engine.py
TOP_K = 5  # Par d√©faut
TOP_K_LARGE_DOCS = 15  # Pour documents volumineux

def _retrieve_context(self, user: User, query: str) -> List[Dict]:
    # D√©tecter si la requ√™te concerne un comptage/agr√©gation
    is_counting_query = any(word in query.lower() for word in
        ['combien', 'nombre', 'total', 'count', 'sum'])

    top_k = TOP_K_LARGE_DOCS if is_counting_query else TOP_K

    chunks_qs = (
        DocumentChunk.objects
        .select_related('document')
        .filter(document__user=user)
        .annotate(distance=CosineDistance('embedding', query_embedding))
        .filter(distance__lte=SIMILARITY_THRESHOLD)
        .order_by('distance')
        [:top_k]
    )
    # ...
```

**B. Ajouter un r√©sum√© au d√©but du document**
- D√©j√† fait avec les statistiques XLSX ‚úÖ
- Permet de r√©pondre aux questions de comptage sans charger tout le document

**C. Index s√©par√© pour m√©tadonn√©es structur√©es** (avanc√©)
- Stocker les statistiques dans un champ JSON du mod√®le Document
- Le RAG consulte d'abord les m√©tadonn√©es avant de chercher dans les chunks

---

## ‚úÖ Plan d'action propos√©

### Phase 1 : Corrections imm√©diates (30 min)

1. ‚úÖ Restaurer les m√©tadonn√©es dans `chunk_text()` avec `filename` en param√®tre
2. ‚úÖ Mettre √† jour `tasks.py` pour passer `document.filename` √† `chunk_text()`
3. ‚úÖ Tester avec un document PDF et un XLSX

### Phase 2 : Tests de r√©gression (15 min)

1. Re-uploader les documents de test
2. V√©rifier que les chunks sont bien cr√©√©s
3. V√©rifier que les questions RAG fonctionnent

### Phase 3 : Documentation (10 min)

1. Mettre √† jour `memoire05_02_26.md` avec les changements
2. Documenter les m√©tadonn√©es disponibles

**Temps total estim√©** : 55 minutes

---

## üîú Prochaines √©tapes (apr√®s SAML)

1. **Migration du mod√®le** : Ajouter `char_count` et `metadata` √† DocumentChunk
2. **OCR optionnel** : Pour les PDF scann√©s
3. **Optimisation TOP_K** : Ajustement dynamique selon le type de requ√™te
4. **Statistiques avanc√©es** : Dashboard admin pour surveiller la qualit√© des chunks

---

## üìù Notes de migration ChromaDB ‚Üí pgvector

**Avantages de pgvector** :
- ‚úÖ Tout dans PostgreSQL (pas de service s√©par√©)
- ‚úÖ Transactions ACID
- ‚úÖ Backup simplifi√©
- ‚úÖ Index HNSW performant

**Inconv√©nients** :
- ‚ùå Pas de stockage natif des m√©tadonn√©es riches (contrairement √† ChromaDB)
- ‚ùå N√©cessite d'enrichir le mod√®le Django pour compenser
- ‚ùå Moins flexible pour des recherches hybrides (texte + vecteur + metadata)

**Conclusion** : La migration est globalement r√©ussie, mais n√©cessite quelques ajustements pour restaurer la richesse des m√©tadonn√©es de l'ancienne version.
